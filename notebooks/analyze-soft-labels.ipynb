{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T15:33:40.907689Z",
     "iopub.status.busy": "2023-02-10T15:33:40.907065Z",
     "iopub.status.idle": "2023-02-10T15:33:48.910064Z",
     "shell.execute_reply": "2023-02-10T15:33:48.908275Z",
     "shell.execute_reply.started": "2023-02-10T15:33:40.907610Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import skweak\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import gc\n",
    "import shutil\n",
    "import evaluate\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tokenizations\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import functional\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import myutils\n",
    "import aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T15:33:50.163316Z",
     "iopub.status.busy": "2023-02-10T15:33:50.162932Z",
     "iopub.status.idle": "2023-02-10T15:33:52.824134Z",
     "shell.execute_reply": "2023-02-10T15:33:52.819840Z",
     "shell.execute_reply.started": "2023-02-10T15:33:50.163271Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/15/bd75q0s16bv5m07mqm04byz80000gn/T/ipykernel_9748/4088028903.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mfile_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'training_ws_lfs.json'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mrubrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmyutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_rubrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_data\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'rubrics.json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mannotated_train_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread_from_json\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_data\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mfile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'read_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "path_data = 'data/'\n",
    "file_name = 'training_ws_lfs.json'\n",
    "rubrics = myutils.load_rubrics(path_data + 'rubrics.json')\n",
    "annotated_train_data = read_from_json(path_data + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T15:33:52.832261Z",
     "iopub.status.busy": "2023-02-10T15:33:52.830878Z",
     "iopub.status.idle": "2023-02-10T15:33:52.889966Z",
     "shell.execute_reply": "2023-02-10T15:33:52.888535Z",
     "shell.execute_reply.started": "2023-02-10T15:33:52.832077Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate stats splitted by German and English\n",
    "stats_de = {}\n",
    "stats_en = {}\n",
    "# initialize stats dict  for the individual labeling_functions\n",
    "for k in annotated_train_data[0]['labeling_functions']:\n",
    "    stats_de[k] ={'class':[], 'labels':[]}\n",
    "for k in annotated_train_data[0]['labeling_functions']:\n",
    "    stats_en[k] ={'class':[], 'labels':[]}\n",
    "\n",
    "for an in annotated_train_data:\n",
    "    c = an['label']\n",
    "    lang = an['lang']\n",
    "    if lang == 'en':\n",
    "        for k,v in an['labeling_functions'].items():\n",
    "            stats_en[k]['labels'].append(v)\n",
    "            stats_en[k]['class'].append(c)\n",
    "    elif lang == 'de':\n",
    "         for k,v in an['labeling_functions'].items():\n",
    "            stats_de[k]['labels'].append(v)\n",
    "            stats_de[k]['class'].append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ANALYSIS START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T15:33:52.892165Z",
     "iopub.status.busy": "2023-02-10T15:33:52.891757Z",
     "iopub.status.idle": "2023-02-10T15:34:03.207399Z",
     "shell.execute_reply": "2023-02-10T15:34:03.205737Z",
     "shell.execute_reply.started": "2023-02-10T15:33:52.892132Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp_de = spacy.load(\"de_core_news_lg\")\n",
    "german_question_ids = [str(i) for i in range(1,9)]\n",
    "th = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analyze individual LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-10T15:34:06.200832Z",
     "iopub.status.idle": "2023-02-10T15:34:06.201329Z",
     "shell.execute_reply": "2023-02-10T15:34:06.201114Z",
     "shell.execute_reply.started": "2023-02-10T15:34:06.201092Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of labels per LF\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "    bin_items = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7,0.8,0.9,1.0]\n",
    "    for k,v in stats.items():\n",
    "        correct, partial_correct, incorrect = [],[],[]\n",
    "        for c, l in zip(v['class'], v['labels']):\n",
    "            if c == 'CORRECT':\n",
    "                correct.append(l)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                partial_correct.append(l)\n",
    "            elif c == 'INCORRECT':\n",
    "                incorrect.append(l)\n",
    "        plt.title(lang + ' ' + k)\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Frequency')\n",
    "        correct = flat_list(correct)\n",
    "        correct = [i for i in correct if i != 0.0]\n",
    "        partial_correct = flat_list(partial_correct)\n",
    "        partial_correct = [i for i in partial_correct if i != 0.0]\n",
    "        incorrect = flat_list(incorrect)\n",
    "        incorrect = [i for i in incorrect if i != 0.0]\n",
    "        plt.hist([correct, partial_correct, incorrect], label=['CORRECT', 'PARTIAL_CORRECT', 'INCORRECT'])\n",
    "        plt.xticks(bin_items)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-10T15:34:06.203376Z",
     "iopub.status.idle": "2023-02-10T15:34:06.204094Z",
     "shell.execute_reply": "2023-02-10T15:34:06.203780Z",
     "shell.execute_reply.started": "2023-02-10T15:34:06.203746Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Share of as relevant labeled tokens based on a threshold (>=0.5)\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "   \n",
    "    for k,v in stats.items():\n",
    "        relevants, unrelevants = [],[]\n",
    "        for labels in v['labels']:\n",
    "            relevant, unrelevant = 0,0\n",
    "            for label in labels:\n",
    "                for l in labels:\n",
    "                    if l > th:\n",
    "                        relevant+=1\n",
    "                    else:\n",
    "                        unrelevant+=1\n",
    "            relevants.append(relevant)\n",
    "            unrelevants.append(unrelevant)\n",
    "        \n",
    "        plt.title(lang + ' ' + k)\n",
    "        plt.pie([np.sum(relevants), np.sum(unrelevants)], labels = ['relevant_token', 'unrelevant_token'],autopct='%1.1f%%')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    print(20*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-10T15:34:06.207589Z",
     "iopub.status.idle": "2023-02-10T15:34:06.208356Z",
     "shell.execute_reply": "2023-02-10T15:34:06.208042Z",
     "shell.execute_reply.started": "2023-02-10T15:34:06.208006Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Share of the general labeled tokens per LF\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "   \n",
    "    for k,v in stats.items():\n",
    "        relevants, unrelevants = [],[]\n",
    "        for labels in v['labels']:\n",
    "            relevant, unrelevant = 0,0\n",
    "            for label in labels:\n",
    "                for l in labels:\n",
    "                    if l > 0.0:\n",
    "                        relevant+=1\n",
    "                    else:\n",
    "                        unrelevant+=1\n",
    "            relevants.append(relevant)\n",
    "            unrelevants.append(unrelevant)\n",
    "        \n",
    "        plt.title(lang + ' ' + k)\n",
    "        plt.pie([np.sum(relevants), np.sum(unrelevants)], labels = ['labeled_token', 'unlabeled_token'],autopct='%1.1f%%')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    print(20*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-10T15:34:06.210257Z",
     "iopub.status.idle": "2023-02-10T15:34:06.211136Z",
     "shell.execute_reply": "2023-02-10T15:34:06.210818Z",
     "shell.execute_reply.started": "2023-02-10T15:34:06.210784Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# labeled tokens per LF per class\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "    data = {'CORRECT': [],'PARTIAL_CORRECT': [], 'INCORRECT': []}\n",
    "    lfs = list(stats.keys())\n",
    "    for k,v in stats.items():\n",
    "        correct, partial_correct, incorrect = [],[],[]\n",
    "        for c, l in zip(v['class'], v['labels']):\n",
    "            if c == 'CORRECT':\n",
    "                correct.append(l)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                partial_correct.append(l)\n",
    "            elif c == 'INCORRECT':\n",
    "                incorrect.append(l)\n",
    "        # flatten and check if relevant -> <= 0.5\n",
    "        correct = flat_list(correct)\n",
    "        correct = len([i for i in correct if i >= th])\n",
    "        partial_correct = flat_list(partial_correct)\n",
    "        partial_correct = len([i for i in partial_correct if i >= th])\n",
    "        incorrect = flat_list(incorrect)\n",
    "        incorrect = len([i for i in incorrect if i > th])\n",
    "        data['CORRECT'].append(correct)\n",
    "        data['PARTIAL_CORRECT'].append(partial_correct)\n",
    "        data['INCORRECT'].append(incorrect)\n",
    "    \n",
    "    plt.title(lang + ' Number of relevant token per LF')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Frequency')    \n",
    "    plt.plot(lfs, data['CORRECT'] ,label='CORRECT')\n",
    "    plt.plot(lfs, data['PARTIAL_CORRECT'] ,label='PARTIAL_CORRECT')\n",
    "    plt.plot(lfs, data['INCORRECT'] ,label='INCORRECT')\n",
    "    plt.xticks(lfs, rotation=90)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-10T15:34:06.213696Z",
     "iopub.status.idle": "2023-02-10T15:34:06.214386Z",
     "shell.execute_reply": "2023-02-10T15:34:06.214087Z",
     "shell.execute_reply.started": "2023-02-10T15:34:06.214055Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How long and how many key elements does a LF detect?\n",
    "for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "    data = {'CORRECT': {'avg_relation': [], 'avg_len_rubrics': [], 'avg_token_per_element': []},'PARTIAL_CORRECT': {'avg_relation': [], 'avg_len_rubrics': [], 'avg_token_per_element': []}, 'INCORRECT': {'avg_relation': [], 'avg_len_rubrics': [], 'avg_token_per_element': []}}\n",
    "    lfs = list(stats.keys())\n",
    "    for k,v in stats.items():\n",
    "        correct, partial_correct, incorrect = [],[],[]\n",
    "        for c, l in zip(v['class'], v['labels']):\n",
    "            if c == 'CORRECT':\n",
    "                correct.append(l)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                partial_correct.append(l)\n",
    "            elif c == 'INCORRECT':\n",
    "                incorrect.append(l)\n",
    "        correct = [silver2target(labels, th) for labels in correct]\n",
    "        #correct = [i for i in correct if i >= th]\n",
    "        partial_correct = [silver2target(labels, th) for labels in partial_correct]\n",
    "        #partial_correct = len([i for i in partial_correct if i >= th])\n",
    "        incorrect = [silver2target(labels, th) for labels in incorrect]\n",
    "        \n",
    "        # dynamic values for every LF\n",
    "        relations, len_rubrics, num_token_per_element = [],[], []\n",
    "        for i,c in enumerate([correct, partial_correct, incorrect]):\n",
    "            rels = [relation(l) for l in c]\n",
    "            len_rubric = [len(rubric_length(l)) for l in c]\n",
    "            token_per_element = [np.average(rubric_length(l)) for l in c]\n",
    "            token_per_element = [t for t in token_per_element if not np.isnan(t)]\n",
    "            avg_relations = np.average(rels)\n",
    "            avg_len_rubrics = np.average(len_rubric)\n",
    "            if len(token_per_element) == 0:\n",
    "                token_per_element.append(0)\n",
    "            avg_token_per_element = np.average(token_per_element)\n",
    "            #relations.append(avg_relations)\n",
    "            #len_rubrics.append(avg_len_rubrics)\n",
    "            #num_token_per_element.append(avg_len_rubrics)\n",
    "            if i == 0: \n",
    "                l = 'CORRECT'\n",
    "            elif i == 1: \n",
    "                l = 'PARTIAL_CORRECT'\n",
    "            elif i == 2: \n",
    "                l = 'INCORRECT'\n",
    "            data[l]['avg_relation'].append(avg_relations)\n",
    "            data[l]['avg_len_rubrics'].append(avg_len_rubrics) \n",
    "            data[l]['avg_token_per_element'].append(avg_token_per_element) \n",
    "    \n",
    "    # TODO adapt it here\n",
    "    plt.xlabel('LFs')\n",
    "    #plt.ylabel() \n",
    "    for l in ['avg_relation', 'avg_len_rubrics', 'avg_token_per_element']:\n",
    "        plt.title(lang + ' Stats ' + l)\n",
    "        plt.plot(lfs, data['CORRECT'][l] ,label='CORRECT')\n",
    "        plt.plot(lfs, data['PARTIAL_CORRECT'][l] ,label='PARTIAL_CORRECT')\n",
    "        plt.plot(lfs, data['INCORRECT'][l],label='INCORRECT')\n",
    "        plt.legend()\n",
    "        plt.xticks(lfs, rotation=90)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aggregation functions\n",
    "Analyze how the different aggregation techniches distribute the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GLOBAL_NORMALIZE = False\n",
    "\n",
    "def global_normalize(data):\n",
    "    # finding min and max value\n",
    "    min_value = 100\n",
    "    max_value = -100\n",
    "    max_vals = []\n",
    "    min_vals = []\n",
    "    for d in data:\n",
    "        v_min = np.min(d)\n",
    "        v_max = np.max(d)\n",
    "        if v_min < min_value:\n",
    "            min_value = v_min\n",
    "            min_vals.append(min_value)\n",
    "        if v_max > max_value:\n",
    "            max_value = v_max\n",
    "            max_vals.append(max_value)\n",
    "    print('max', max_vals, 'min', min_vals)\n",
    "    range_val = max_value - min_value\n",
    "    data_norm = [[(l - min_value) / range_val for l in d] for d in data]\n",
    "    return data_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# different aggregation functions\n",
    "# exclude all hard match functions\n",
    "#exclude = ['LF_word_alginment', 'LF_dep_match_without_stopwords', 'LF_dep_match', 'LF_tag_match', 'LF_stem_match', 'LF_pos_match_without_stopwords', 'LF_lemma_match_without_stopwords','LF_pos_match' , 'LF_pos_match', 'LF_noun_phrase']\n",
    "exclude=[]\n",
    "#exclude = ['LF_dep_match_without_stopwords', 'LF_dep_match', 'LF_tag_match','LF_pos_match_without_stopwords', 'LF_lemma_match_without_stopwords', 'LF_pos_match', 'LF_noun_phrases' ]\n",
    "#exclude = ['LF_parahrase_detection_candidates', 'LF_parahrase_detection_sentences', 'LF_bleu_candidates', 'LF_jaccard_similarity', 'LF_edit_distance']\n",
    "annotations_train = extract_annotations(annotated_train_data, exclude_LFs=exclude)\n",
    "\n",
    "#annotations_train = extract_annotations_only_LFs(annotated_train_data, lfs=['LF_parahrase_detection_sentences', 'LF_parahrase_detection_candidates'])\n",
    "#annotations_dev = extract_annotations_only_LFs(annotated_dev_data, lfs=['LF_parahrase_detection_sentences', 'LF_parahrase_detection_candidates'])\n",
    "\n",
    "for lang in ['de', 'en']:\n",
    "    for mode in ['average', 'max', 'average_nonzero', 'sum']:\n",
    "        stats = {\n",
    "            'CORRECT':[],\n",
    "            'PARTIAL_CORRECT': [],\n",
    "            'INCORRECT': []\n",
    "        }\n",
    "        labels = []\n",
    "        for a, an in zip(annotations_train, annotated_train_data):\n",
    "            y = aggregate_soft_labels(a, mode)\n",
    "            if not GLOBAL_NORMALIZE:\n",
    "                y = y#normalize(y)\n",
    "            labels.append(y)\n",
    "        if GLOBAL_NORMALIZE:\n",
    "            labels = global_normalize(labels)\n",
    "        print(mode, GLOBAL_NORMALIZE)\n",
    "        for an, label in zip(annotated_train_data, labels):\n",
    "            c = an['label']\n",
    "            l = an['lang']\n",
    "            if l  == lang:\n",
    "                #for mode in ['average', 'max', 'average_nonzero', 'sum']:\n",
    "                stats[c].append(label)\n",
    "        \n",
    "        bin_items = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        #counts, bins = np.histogram(x,bins=bin_items)\n",
    "        plt.title(lang.upper() + ' ' + mode + '_global_norm='+ str(GLOBAL_NORMALIZE))\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Frequency')\n",
    "        correct = flat_list(stats['CORRECT'])\n",
    "        partial_correct = flat_list(stats['PARTIAL_CORRECT'])\n",
    "        incorrect = flat_list(stats['INCORRECT'])\n",
    "        normalized_data = []\n",
    "        weights = []\n",
    "        \n",
    "        for d in [correct, partial_correct, incorrect]:\n",
    "            w = np.ones_like(d) / len(d)\n",
    "            weights.append(w)\n",
    "        plt.hist([correct, partial_correct, incorrect], bins=bin_items, weights=weights, label=['CORRECT', 'PARTIAL_CORRECT', 'INCORRECT'])\n",
    "        plt.xticks(bin_items)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# different aggregation functions\n",
    "# exclude all hard match functions\n",
    "#exclude = ['LF_word_alginment', 'LF_dep_match_without_stopwords', 'LF_dep_match', 'LF_tag_match', 'LF_stem_match', 'LF_shape_match', 'LF_pos_match_without_stopwords', 'LF_lemma_match_without_stopwords', 'LF_pos_match', 'LF_lemma_match', 'LF_noun_phrase']\n",
    "exclude=[]\n",
    "#exclude = ['LF_dep_match_without_stopwords', 'LF_dep_match', 'LF_tag_match','LF_pos_match_without_stopwords', 'LF_lemma_match_without_stopwords', 'LF_pos_match', 'LF_noun_phrases' ]\n",
    "#exclude = ['LF_parahrase_detection_candidates', 'LF_parahrase_detection_sentences', 'LF_bleu_candidates', 'LF_jaccard_similarity', 'LF_edit_distance']\n",
    "annotations_train = extract_annotations(annotated_train_data, exclude_LFs=exclude)\n",
    "#annotations_train = extract_annotations_only_LFs(annotated_train_data, lfs=['LF_parahrase_detection_sentences', 'LF_parahrase_detection_candidates'])\n",
    "#annotations_dev = extract_annotations_only_LFs(annotated_dev_data, lfs=['LF_parahrase_detection_sentences', 'LF_parahrase_detection_candidates'])\n",
    "\n",
    "def global_normalize(data):\n",
    "    # finding min and max value\n",
    "    min_value = 100\n",
    "    max_value = -100\n",
    "    max_vals = []\n",
    "    min_vals = []\n",
    "    for d in data:\n",
    "        v_min = np.min(d)\n",
    "        v_max = np.max(d)\n",
    "        if v_min < min_value:\n",
    "            min_value = v_min\n",
    "            min_vals.append(min_value)\n",
    "        if v_max > max_value:\n",
    "            max_value = v_max\n",
    "            max_vals.append(max_value)\n",
    "    print('max', max_vals, 'min', min_vals)\n",
    "    #min_value = min_vals[0]\n",
    "    #max_value = max_vals[0]\n",
    "    #min_value = np.average(min_vals)\n",
    "    #max_value = np.average(max_vals)\n",
    "    range_val = max_value - min_value\n",
    "    data_norm = [[(l - min_value) / range_val for l in d] for d in data]\n",
    "    return data_norm\n",
    "\n",
    "for mode in ['average', 'max', 'average_nonzero', 'sum']:\n",
    "        stats = {\n",
    "            'CORRECT':[],\n",
    "            'PARTIAL_CORRECT': [],\n",
    "            'INCORRECT': []\n",
    "        }\n",
    "        labels = []\n",
    "        for a, an in zip(annotations_train, annotated_train_data):\n",
    "            y = aggregate_soft_labels(a, mode)\n",
    "            y= normalize(y)\n",
    "            labels.append(y)\n",
    "        for an, label in zip(annotated_train_data, labels):\n",
    "            c = an['label']\n",
    "            \n",
    "            #for mode in ['average', 'max', 'average_nonzero', 'sum']:\n",
    "            stats[c].append(label)\n",
    "        \n",
    "        bin_items = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        #counts, bins = np.histogram(x,bins=bin_items)\n",
    "        plt.title(mode)\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Frequency')\n",
    "        correct = flat_list(stats['CORRECT'])\n",
    "        partial_correct = flat_list(stats['PARTIAL_CORRECT'])\n",
    "        incorrect = flat_list(stats['INCORRECT'])\n",
    "        normalized_data = []\n",
    "        weights = []\n",
    "        \n",
    "        for d in [correct, partial_correct, incorrect]:\n",
    "            w = np.ones_like(d) / len(d)\n",
    "            weights.append(w)\n",
    "        plt.hist([correct, partial_correct, incorrect], bins=bin_items, weights=weights, label=['CORRECT', 'PARTIAL_CORRECT', 'INCORRECT'])\n",
    "        plt.xticks(bin_items)\n",
    "        plt.legend()\n",
    "            #n, bins = np.histogram(d, bins=bin_items,density=True)\n",
    "            #n = n / n.sum()\n",
    "            #print(n)\n",
    "            #normalized_data.append(n)\n",
    "            #plt.bar(normalized_data, bins=bin_items)\n",
    "        #plt.hist(normalized_data[1],ins=bin_items)\n",
    "        #plt.hist(normalized_data[2], ins=bin_items)\n",
    "       \n",
    "        #data = np.concatenate([correct, partial_correct, incorrect])\n",
    "        #n, bins = np.histogram(data, bins=bin_items,density=True)\n",
    "        # Normalize the histogram\n",
    "        #n = n / n.sum()\n",
    "        #print(n)\n",
    "        #plt.hist(n, bins=bin_items, )\n",
    "        # Plot the normalized histogram\n",
    "        #plt.bar(bin_items, n[0], width=0.05)\n",
    "        #plt.bar(bin_items, n[1], width=0.05)\n",
    "        #plt.bar(bin_items, n[2], width=0.05)\n",
    "\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analyze Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "german_ids = [str(i) for i in range(1,10)]\n",
    "\n",
    "x_de= []\n",
    "x_en= []\n",
    "for k in rubrics.keys():\n",
    "    if k in german_ids:\n",
    "        x_de.append(len(rubrics[k]['key_element']))\n",
    "    else:\n",
    "        x_en.append(len(rubrics[k]['key_element']))\n",
    "\n",
    "print('Average key elements DE', np.average(x_de))\n",
    "print('Average key elements EN', np.average(x_en))\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.title('Key elements in rubrics')\n",
    "plt.hist([x_de, x_en], label=['DE', 'EN'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analyze Annotated Key Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for language in ['de', 'en']:\n",
    "    relations_correct, relations_partial, relations_incorrect = [], [],[]\n",
    "    len_rubrics_correct, len_rubrics_partial, len_rubrics_incorrect = [], [],[]\n",
    "    len_rubrics_average_correct, len_rubrics_average_partial, len_rubrics_average_incorrect = [], [],[]\n",
    "    for an, labels in zip(annotated_train_data, silver_labels):\n",
    "        lang = an['lang']\n",
    "        c = an['label']\n",
    "        if lang == language:\n",
    "            true_labels = silver2target(labels,th=th)\n",
    "            rel = relation(true_labels)\n",
    "            len_rubrics = rubric_length(true_labels)\n",
    "            if len(len_rubrics) > 0:\n",
    "                len_rubrics_average = np.average(len_rubrics)\n",
    "            else:\n",
    "                len_rubrics_average = 0\n",
    "            if c == 'CORRECT':\n",
    "                relations_correct.append(rel)\n",
    "                len_rubrics_correct.append(len(len_rubrics))\n",
    "                len_rubrics_average_correct.append(len_rubrics_average)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                relations_partial.append(rel)\n",
    "                len_rubrics_partial.append(len(len_rubrics))\n",
    "                len_rubrics_average_partial.append(len_rubrics_average)\n",
    "            elif c == 'INCORRECT':\n",
    "                relations_incorrect.append(rel)\n",
    "                len_rubrics_incorrect.append(len(len_rubrics))\n",
    "                len_rubrics_average_incorrect.append(len_rubrics_average)\n",
    "    \n",
    "    print(language.upper() +' Relation:','CORRECT', np.average(relations_correct), 'PARTIAL_CORRECT', np.average(relations_partial),'INCORRECT', np.average(relations_incorrect))\n",
    "    \n",
    "    plot_hist({'CORRECT': len_rubrics_average_correct, 'PARTIAL_CORRECT': len_rubrics_average_partial, 'INCORRECT': len_rubrics_average_incorrect}, bins=range(0,50,5), title=language.upper()+' Number of tokens in key element')         \n",
    "    print(language.upper() + ' Average len (tokens):', 'CORRECT',np.average(len_rubrics_average_correct),'PARTIAL_CORRECT', np.average(len_rubrics_average_partial), 'INCORRECT', np.average(len_rubrics_average_incorrect))\n",
    "    \n",
    "    plot_hist({'CORRECT': len_rubrics_correct, 'PARTIAL_CORRECT': len_rubrics_partial, 'INCORRECT': len_rubrics_incorrect}, bins=range(0,20,2), title=language.upper() + ' Number of key elements in answer')         \n",
    "    print(language.upper() + ' Average number of rubrics in answer:', 'CORRECT',np.average(len_rubrics_correct), 'PARTIAL_CORRECT',np.average(len_rubrics_partial),'INCORRECT', np.average(len_rubrics_incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for question_id in np.unique(list(rubrics.keys())):\n",
    "    relations_correct, relations_partial, relations_incorrect = [], [],[]\n",
    "    len_rubrics_correct, len_rubrics_partial, len_rubrics_incorrect = [], [],[]\n",
    "    len_rubrics_average_correct, len_rubrics_average_partial, len_rubrics_average_incorrect = [], [],[]\n",
    "    for an, labels in zip(annotated_train_data, silver_labels):\n",
    "        q_id = an['question_id']\n",
    "        c = an['label']\n",
    "        if q_id == question_id:\n",
    "            true_labels = silver2target(labels)\n",
    "            rel = relation(true_labels)\n",
    "            len_rubrics = rubric_length(true_labels)\n",
    "            if len(len_rubrics) > 0:\n",
    "                len_rubrics_average = np.average(len_rubrics)\n",
    "            else:\n",
    "                len_rubrics_average = 0\n",
    "            if c == 'CORRECT':\n",
    "                relations_correct.append(rel)\n",
    "                len_rubrics_correct.append(len(len_rubrics))\n",
    "                len_rubrics_average_correct.append(len_rubrics_average)\n",
    "            elif c == 'PARTIAL_CORRECT':\n",
    "                relations_partial.append(rel)\n",
    "                len_rubrics_partial.append(len(len_rubrics))\n",
    "                len_rubrics_average_partial.append(len_rubrics_average)\n",
    "            elif c == 'INCORRECT':\n",
    "                relations_incorrect.append(rel)\n",
    "                len_rubrics_incorrect.append(len(len_rubrics))\n",
    "                len_rubrics_average_incorrect.append(len_rubrics_average)\n",
    "    \n",
    "    print(question_id +' Relation:','CORRECT', np.average(relations_correct), 'PARTIAL_CORRECT', np.average(relations_partial),'INCORRECT', np.average(relations_incorrect))\n",
    "    \n",
    "    plot_hist({'CORRECT': len_rubrics_average_correct, 'PARTIAL_CORRECT': len_rubrics_average_partial, 'INCORRECT': len_rubrics_average_incorrect}, bins=range(0,50,5), title=question_id+' Number of tokens in key element')         \n",
    "    print(question_id + ' Average len (tokens):', 'CORRECT',np.average(len_rubrics_average_correct),'PARTIAL_CORRECT', np.average(len_rubrics_average_partial), 'INCORRECT', np.average(len_rubrics_average_incorrect))\n",
    "    \n",
    "    plot_hist({'CORRECT': len_rubrics_correct, 'PARTIAL_CORRECT': len_rubrics_partial, 'INCORRECT': len_rubrics_incorrect}, bins=range(0,20,2), title=question_id + ' Number of key elements in answer')         \n",
    "    print(question_id+ ' Average number of rubrics in answer:', 'CORRECT',np.average(len_rubrics_correct), 'PARTIAL_CORRECT',np.average(len_rubrics_partial),'INCORRECT', np.average(len_rubrics_incorrect))\n",
    "    print(30*'-' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysis Dataset\n",
    "Analyze the SAF dataset itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stats={'en':0, 'de':0}\n",
    "for lang in X_train['lang']:\n",
    "    stats[lang] += 1\n",
    "plt.title('Dataset DE/EN split')\n",
    "plt.pie([stats['de'], stats['en']], labels = list(stats.keys()),autopct='%1.1f%%')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Combined Plot\n",
    "c_stats = {'CORRECT':[],'PARTIAL_CORRECT':[],'INCORRECT':[] }\n",
    "stats = {'de': c_stats, 'en': c_stats}\n",
    "for an in annotated_train_data:\n",
    "    lang = an['lang']\n",
    "    student_answer = an['student_answer']\n",
    "    label = an['label']\n",
    "    inputs = tokenizer.encode(student_answer, return_special_tokens_mask=False)\n",
    "    len_seq = len(inputs)-2 # ignore cls and sep token\n",
    "    stats[lang][label].append(len_seq)\n",
    "    \n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.title('Length of student answer')\n",
    "plt.hist([stats['de']['CORRECT'], stats['de']['PARTIAL_CORRECT'],stats['de']['INCORRECT'],stats['en']['CORRECT'], stats['en']['PARTIAL_CORRECT'],stats['en']['INCORRECT']], label=['de_correct', 'de_partial_correct', 'de_incorrect', 'en_correct', 'en_partial_correct', 'en_incorrect'], bins=range(0,300, 20))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Separate plots\n",
    "for language in ['en', 'de']:\n",
    "    stats = {'CORRECT':[],'PARTIAL_CORRECT':[],'INCORRECT':[]}\n",
    "    for an in annotated_train_data:\n",
    "        lang = an['lang']\n",
    "        student_answer = an['student_answer']\n",
    "        label = an['label']\n",
    "        if lang == language:\n",
    "            inputs = tokenizer.encode(student_answer, return_special_tokens_mask=False)\n",
    "            len_seq = len(inputs)-2 # ignore cls and sep token\n",
    "            stats[label].append(len_seq)\n",
    "\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Occurrences')\n",
    "    plt.title(language.upper()+' Length of student answer')\n",
    "    plt.hist([stats['CORRECT'], stats['PARTIAL_CORRECT'],stats['INCORRECT']], label=list(stats.keys()), bins=range(0,300, 20))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(language.upper()+' Class split')\n",
    "    plt.pie([len(stats['CORRECT']), len(stats['PARTIAL_CORRECT']), len(stats['INCORRECT'])], labels = list(stats.keys()),autopct='%1.1f%%')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "questions = [x['question_id'] for x in annotated_train_data]\n",
    "c = Counter(questions)\n",
    "print(c)\n",
    "plt.title('Dataset question share')\n",
    "\n",
    "plt.pie(list(c.values()), labels = list(c.keys()))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stats={'en':0, 'de':0}\n",
    "langs = [x['lang'] for x in annotated_train_data]\n",
    "for lang in langs:\n",
    "    stats[lang] += 1\n",
    "plt.title('Dataset DE/EN split')\n",
    "plt.pie([stats['de'], stats['en']], labels = list(stats.keys()),autopct='%1.1f%%')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# What are the words that a LF detect \n",
    "from collections import Counter\n",
    "\n",
    "def get_annotated_text_per_LF(is_word_level=False, th=0.5):\n",
    "    result = {}\n",
    "    for lang, stats in {'DE': stats_de, 'EN': stats_en}.items():\n",
    "        result[lang] = {}\n",
    "        for k in stats.keys():\n",
    "            labeled_tokens = []\n",
    "            for text, labels, qid in zip(X_train['student_answer'] ,stats[k]['labels'], X_train['question_id']):\n",
    "                # tokenize respective the language\n",
    "                if qid in german_question_ids:\n",
    "                    tokens = nlp_de(text)\n",
    "                else:\n",
    "                    tokens = nlp(text)\n",
    "                hard_labels = silver2target(labels, th=th)\n",
    "                indicies = get_idxs_elements(hard_labels)\n",
    "                labeled_tokens.append([tokens[i[0]:i[1]] for i in indicies])\n",
    "            labeled_tokens = flat_list(labeled_tokens)\n",
    "            if is_word_level:\n",
    "                labeled_tokens = [t.text for doc in labeled_tokens for t in doc]\n",
    "            c = Counter(labeled_tokens)\n",
    "            result[lang][k] = sorted(dict(c.most_common(10)).items(), key=lambda pair: pair[1], reverse=True)\n",
    "    return result\n",
    "\n",
    "result_word_level = get_annotated_text_per_LF(is_word_level=True,th=th)\n",
    "for i, result in enumerate([result_word_level]):\n",
    "    if i == 0: print('WORD LEVEL STATS') \n",
    "    else: print('PHRASE LEVEL STATS') \n",
    "    for lang in ['DE', 'EN']:\n",
    "        lfs = result[lang].keys()\n",
    "        for lf in lfs:\n",
    "            print(lf)\n",
    "            print(result[lang][lf])\n",
    "            print(20*'-')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}